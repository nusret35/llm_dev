1 18.409 An Algorithmist’s Toolkit Septem ber 29, 2009 
Lecture 6 
Lecturer: Jonathan Kelner Scribe: Anthony Kim(2009) 
Topics 
• Diameters and their relationship to λ2 
• Expanders 
• Butterﬂy networks 
Diameters and Eigen values 
So far, every time we’ve dealt with eigenvalues, it’s had something to do with connectivit y. For example, 
the spectral gap can be used to approximate the qualit y of cuts; it also describ es how well a graph can mix 
under a random walk. They both are saying similar things: the eigenvalue is saying how connected a graph 
is. A walk will mix quickly if there’s a lot connected to everything else. The min-cut will likewise be large 
if there’s a lot of connectivit y. 
For almost every reasonable property about a graph, there’s something you can write down regarding its 
relation to the second eigenvalue of the Laplacian. Today, we’re going to show the relation between λ2 and 
the diameter of a graph. 
Deﬁnition 1 The diameter, δ, is the longest, shortest path between any two vertices of a graph. In other 
words, we can deﬁne the distanc e between two vertices u and v in G as the shortest path connecting the two. 
The diameter of G is the largest distanc e between any two vertices in G. 
It’s not immediately clear why the diameter should be related to λ2; the following provides intuition1: 
1. Well-connected graphs have big λ2 
2. Well-connected graphs have a small δ 
3. So, graphs with big λ2 should have small δ 
Before proceeding, we’ll be making the following assumption: 
Assumption 2 G is a d-regular graph (this is for simplicity and not really a limiting assumption). 
We’ll also be utilizing lazy random walks in our investigation. As a reminder, 
Deﬁnition 3 A lazy random walk is simply a random walk along a graph with self loops added in: 
A I M = + (1)2d 2 /bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright /bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright 
Random Walk Self-lo ops 
where A is the graph’s adjacency matrix and I is the identity matrix. 
Since we’re using adjacency matrices, the interesting eigenvalues will be close to 1. So, let μ2 be the 
second largest eigenvalue of M and λ be the gap (i.e., λ =1 − μ2). 
1Note, however, that this is not a prop er syllogism 
6-1 /radicalBigg 
/summationdisplay 
/summationdisplay 1.1 A First Bound 
Claim 4 For any G, 
ln(n)δ ≤ (2)λ 
where δ is the diameter of G, n is the number of vertices in the graph, and λ =1 − μ2, where μ2 is the 
second largest eigenvalue of the M associated with G. 
Claim 4 means that λ, up to a logarithmic factor, really does provide a direct bound on the diameter of 
the graph. For most graphs, this isn’t very tight, but it’s a good place to start. So, why is it true? 
Pro of We will use random walks to prove claim 4. Let u and v be vertices that are as far as possible from 
one another . Start a random walk at u and let pt(v) be the probabilit y of a walk being at vertex v at time 
t.If pt(v) > 0, then δ ≤ t. Intuitively, this means that if we start at u and, after t time steps, there is some 
probabilit y of ending up at v (the farthest vertex from u), then there must be a path of length t between 
the two. If there wasn’t, pt(v) would be 0. 
Recalling that the stationary distribution is π(v)=1/n for regular graphs, we can equivalently state that 
if |pt(v) − π(v)| < 1 , then δ ≤ t. Why? Because if |pt(v) − π(v)| < 1 , then pt(v) > 0, implying δ ≤ t. n n 
Recall from our earlier lecture on random walks that 
|pt(v) − π(v)| < (1 − λ)t 
mind
y (v
d)
(y) =(1 − λ)t 
Since G is regular, d(v)= d for all vertices (allowing the last equalit y above). We’ll now look at what 
happens when we set t = ln 
λn : /parenleftbigg/parenrightbiggln n 
ln n 1 1 
λ (1 − λ)t =(1 − λ) < = e n 
With the inequalit y coming from the fact that (1 − λ)1/λ < 1 
e for all λ> 0. Thus, for t = ln 
λn , we have that 
|pt(v) − π(v)| < 1 , and therefore, δ ≤ t = ln n . n λ 
1.2 A better bound 
As stated earlier, bounding δ by n 1 is not that great. So, can we do better? Yes. And we do so by using 
an important trick that frequen tly comes up. First, note that if the (u,v)th entry of Ak is non-zero, then 
there’s a path of at most length k from u to v. Replacing A with M doesn’t change this (it just makes the 
non-zero entries smaller). If eu and ev are basis vectors, then 
11 |pk(v) − π(v)| = |e T Mk −| <v eu nn 
which would imply δ ≤ k. Let’s then let p(x) be a polynomial of degree k: 
k 
p(x)= cj xj 
j=1 
Note that we can also interpret M as a variable and apply p to it as follows: 
k 
p(M)= cj Mj 
j=1 
If p(M) has no zero entries, then δ ≤ k. Why? Because all non-zero elemen ts of M indicate all vertices 
that can be reached in one step, M2 is all vertices that can be reached in two steps, and so on. Thus, 
6-2 /summationdisplay 
/summationdisplay 
/summationdisplay /summationdisplay /summationdisplay /summationdisplay /summationdisplay /summationdisplay 
/parenleftBigg /summationdisplay /parenrightBigg 
/summationdisplay 
/summationdisplay 
/summationdisplay 
/vextendsingle /vextendsingle /vextendsingle /vextendsingle /vextendsingle /summationdisplay /vextendsingle /vextendsingle /vextendsingle /vextendsingle /vextendsingle 
/summationdisplay 
/summationdisplay 
/summationtext for any non-zero entry (u,v)in p(M), there must have been a non-zero elemen t in Mj for 0 <j ≤ k, 
implying the existence of a path from u to v of at most length k. If this is true for all entries in p(M), then 
a path of at most length k exists from any vertex to any other vertex, which means the diameter is at most k. 2 
Claim 5 Suppose p has degree k, p(1)=1, and |p(μi)|< 1 for all i ≥2, then n 
δ ≤k (3) 
Pro of For this proof, it is suﬃcien t to show that every entry in p(M) is non-zero. First, recall that we 
can write down any matrix, M, as the following: 
M = μiviviT 
i 
where μi and vi are the i-th eigenvalue and eigenvector, respectively. Since 
Mk = μk
i viviT 
i 
we can write 
⎞ ⎛ 
k k k 
p(M)= cj Mj = cj μj 
i viv T 
i =⎝cj μj 
i ⎠ viv T = T p(μi)vivi i 
j=0 j=0 i i j=0 i 
Therefore, we can write the (a,b)th entry of p(M) as follows 
e T
a p(M)eb = e T
a p(μi)viviT eb 
i 
= p(μi)(e T
a vi)(viT eb) 
i 
= p(μi)vi(a)vi(b) 
i 
n1 + p(μi)vi(a)vi(b) = n i=2 
n1 ≥ − n p(μi)vi(a)vi(b) 
i=2 
n1 |p(μi)||vi(a)||vi(b)| ≥ − n i=2 
n 
≥ 1 −max |p(μi)|n i≥2 |vi(a)||vi(b)|
i=2 
≥ 1 
n −max 
i≥2 |p(μi)| 
> 0 
Where the penultimate step follows from |vi(a)||vi(b)|≤ 1. Let V be the matrix where rows are the i 
eigenvectors vi’s. Then V ·V T = I by the orthonormal condition. It follows that V T ·V = I and the columns 
2Note that we’re not saying anything about δ if p(M) has zero entries. Since there are no restrictions on cj , it’s possible 
that the summation produces a zero entry for p(M) where for all positiv e cj a non-zero entry would have existed. 
6-3 /vextendsingle /vextendsingle 
/parenleftBig /parenrightBig n n nof V form an orthonormal basis. Hence ( /summationtext|vi(a)||vi(b)|)2 ≤ /parenleftbig/summationtext|vi(a)|2 /parenrightbig/parenleftbig/summationtext|vi(b)|2 /parenrightbig 
≤ 1. Thei=2 i=2 i=2
ultimate step follows from our assumption that |p(μi)| < 1 for all i ≥ 2. Thus, if p(1)=1 and |p(μi)| < 1 
n n 
for all i≥ 2, we have that every entry in p(M) is non-zero, implying that δ ≤ k. 
Claim 6 For any t ∈ (0,1), I assert the existenc e of a magic polynomial, pk (t), with the following properties: 
1. p(
kt) is of degree k 
2. pk (t)(1)=1 
3. /vextendsingle/vextendsingle 
p(
kt)(x)/vextendsingle/vextendsingle 
≤ 2 /parenleftbig 
1+ √ 
2t /parenrightbig−k for any x∈ [0,1 − t] 
We will provide no proof for this claim here, but the polynomials are derived from Chebyshev polynomials, 
and we’ll use them again later. To provide some intuition, ﬁgure 1 shows graphs of these polynomials for 
k = 10 with varying t. Notice that to keep the same bound for smaller values of t, a larger k is required due 
to the “oscillations” that the polynomial must take on in order to achieve p(1) = 1 while keeping p(x) small 
for x∈ [0,1 − t]. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 
0.99 0.991 0.992 0.993 0.994 0.995 0.996 0.997 0.998 0.999 1 −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 
(a) (b) (c) 
Figure 1: (a) t=0.1 (b) t=0.001 (c) t=0.001 zoomed in for x from 0.99 to 1 
If we set t= λ, we get a degree k polynomial, p, such that 
1. p(1)=1 
/parenleftBig √ /parenrightBig−k 
2. |p(x)|≤ 21+ 2λ for any x∈ [0,μ2]. 
Additionally , if we set k = 1+ √1 ln(2n), then it is possible to show that p(x) < 1 for all x ∈ [0,μ2],2λ n 
which gives the following bound: /parenleftbigg /parenrightbigg1 δ ≤ 1+ √ ln(2n) (4)
2λ 
This is much better than our previous bound of δ ≤ ln(n) . So, strangely , by putting in a particular √ λ 
polynomial, we get a bound that grows with 1/λ as opposed to just 1/λ. This foreshado ws our next unit 
on iterativ e linear algebra. 
1.3 Example Application 
Suppose that you have a symmetric matrix M and want the eigenvector associated with the largest eigenvalue. 
For purposes of this example, let them be normalized such that the largest eigenvalue is 1. Then an easy 
way to get an approximate answer for the eigenvector is to compute Mkx for a large k and a random x. 
6-4 /radicalBigg Why does this give the eigenvector associated with μ1 = 1? If all other eigenvalues are less than 1, then for 
a large enough k, they will diminish in importance, until all that is left is v1. This is a very intuitive and 
natural algorithm that takes about 1/λ steps to get close. 
But we just found a much faster algorithm! Assuming that we know some good bound on λ (if we don’t, 
we could easily search for it), we can compute pk (λ)(M)x instead of Mkx to get the dominan t eigenvector. 
This metho d converges much faster, and we’ll get into this more in a few lectures. 
2 Expanders 
If you had to know one set of graphs in your life, these are the ones to know. They often are a counterexample 
to many long-standing conjectures. Also, they turn up literally everywhere. If you didn’t know any better, 
you would think that they don’t exist from the describ ed properties. But they’re almost every single graph. 
Speciﬁcally , we’ll be looking at families of d-regular graphs (Gn)n as n goes to inﬁnit y: 
Deﬁnition 7 (Gn)n is an expander family if λ2(Gn) ≥c for some constant c and for all n. 
Most of the graphs we’ve looked at are not expanders. For example, path graphs have λ2 ≤O(1/n2) 
and binary trees have λ2 ≤O(1/n). This means that λ2 very quickly goes to zero as n →∞ for both cases. 
Expanders don’t have this property. Even as n →∞, λ2 stays above a constan t. Given this, it’s not clear 
that they should exist. 
Note: We should think of d as a constan t. In other words, we’ll pick a d and study expanders in that family . 
2.1 Relating Expanders to Cuts 
The ﬁrst thing we’ll look at is Cheeger’s inequalit y for expanders. Recall that 
λ2 ≤φ(G)2 
For expanders, this implies c ≤φ(G)2 
What does this mean? It means that any set, S, of vertices with |S|≤n/2 has at least (c/2)|S|edges 
leaving it. This is a strong property: for expanders, there are no small cuts that can be made in the graph. 
Every cut that balances the sizes of the sets of vertices cuts a constan t fraction of the edges in the graph. 
The other side of Cheeger’s inequalit y says 
φ(G)2 
Θ(1) ≤λ2d 
Again, for expanders, this can be rewritten. 
cd φ(G) ≤ 2Θ(1) 
Since d is a constan t, this says that the isoperimetry , φ, is also bounded above by a constan t. Nor­
mally, there’s a large gap between the upper-bound and lower-bounds in Cheeger’s inequalit y. Here we’ve 
sandwic hed φ between two constan ts. Therefore, an equivalent deﬁnition of expanders is as follows: 
Deﬁnition 8 (Gn)n is expander family if φ(G) ≥c/prime for some constant c/prime and all n. 
6-5 