/parenleftbigg /parenrightbigg/parenleftbigg /parenrightbigg /parenleftbigg /parenrightbigg 18.409 An Algorithmist’s Toolkit Decem ber 1, 2009 
Lecture 22 
Lectur er: Jonathan Kelner 
1 Last time 
Last time, we reduced solving sparse systems of linear equations Ax = b where A is symmetric and positive 
deﬁnite to minimizing the quadratic form f(x)= 1 xT Ax − bx + c.2 
The idea of steepest descen t is to pick a point, ﬁnd the direction of steepest decrease, step along that 
direction, and iterate until we get close to a zero. The appropriate directions turn out to be the residuals. 
We pick the step length to take us to the minimal f value along the line. 
Each iteration requires only two matrix-v ector multiplications. We can push this down to one by calcu­
lating the residuals as 
ri+1 = b − Axi+1 
= b − A(xi + αiri) 
=(b − Axi) − αiAri 
= ri − αiAri 
This can allow ﬂoating point error to accum ulate, though that can be ﬁxed by occasionally calculating 
the residual using the original formula. 
Today we’ll talk about how to bound the error, and later how to get better performance. 
2 Con vergence Analysis of Steep est Descen t 
We study a very similar metho d that doesn’t do the line search – it uses the same step length α at every 
iteration. Asymptotically , the best possible α and usual steepest descen t have the same behavior. 
Steepest descen t doesn’t depend on the basis at all, so we might as well pick the eigenbasis for analysis. 
The size of the error at each step certainly won’t change if we switch bases, and it will be easier to see what’s 
going on. For further cleanliness everything will be stated for 2x2 matrices – everything generalizes. 
If we were trying to solve 
λ1 0 x1 = b1 
0 λ2 x2 b2 
the exact solution would be xi = λ1 
i b. Keep in mind that we’re working in the eigenbasis to do the analysis, 
but the actual algorithm does not get to work in the basis, since ﬁnding the eigenbasis is as hard as inverting 
the matrx. 
First, let’s see what happens if we use α = 1. Obviously this is not general, for example if the λs are 
greater than 2, but the algebra is enligh tening. 
Let’s start with x0 = 0, so the ﬁrst residual r0 = b, and for i> 0 the residual will be 
ri = ri−1 − Ari−1 
=(1 − A)ri−1 
=(1 − A)ib 
22-1 /bracketleftBigg /bracketrightBigg /summationdisplay 
/bracketleftBigg /bracketrightBigg /summationdisplay 
/parenleftbig where the last step follows from induction. Since xi = xi−1 + ri = /summationtext
ki 
=0 rk, we can now write 
k−1 
xk = (1 − A)i b 
i=0 
/summationtextk−1But the sum i=0 (1 − A)i is just the ﬁrst k terms of the taylor series 1/y around 1 – that is, xk estimates 
1 b using Taylor series! λi For α /negationslash= 1 the computation is similar, 
k−1 
xk = α(1 − αA)i b 
i=0 
and we get another Taylor series approximation: 1/y around 1/α. 
So how well can we choose α? We want the residuals to go to zero quickly. If we had 1 × 1 matrices, 
we could just set α = λ 1 and get residual 0 in one step, but in general we need to choose α which works for 
diﬀeren t eigenvalues simultaneously . 
Taylor series only converge well very near where you expand it; this gives some intuition for why the 
condition number should be related to the distance between λmax and λmin. If these eigenvalues are far 
apart, then there is no α that works for all the eigenvalues. 
We can bound the L2 norm of the residual by bounding bi by ||b||2 and taking the max of the multipliers 
||rk||2 ≤ max i|1 − αλi|k||b||2 
So, we want to minimize 
max i|1 − αλi| 
Since the maxim um will occur at either the largest or the smallest eigenvalue, the best we can do is to 
balance them and have (1 − αλmin)= −(1 − αλmin). This gives that the best α is the recipro cal of the 
midrange of the eigenvalues: 
λmin+λmax /parenrightbig−1 α = 2 
The resulting max i|1 − αλi| is 1 − 2 where κ = λmax/λmin, which we call the condition numb er of A.κ+1 
Note that κ is a ratio of eigenvalues, so it’s unchanged by scaling the matrix. 
From the bound for the L2 norm, we can derive that the number of iterations grows linearly in κ.Now 
can we do better? 
3 Conjugate Directions 
Curren tly we are going to the minimal of f value along our search direction. As we saw in previous example, 
this can us to take a long zigzag path. What we would really like to do is go the length of the projection 
of x onto our search direction. If we could do that, then after i steps the error would be orthogonal to all 
previous search directions, and we’d be done with an n × n matrix after n iterations. 
Suppose we have orthogonal directions d0,...,d n−1 – the standard basis will do.
We have xi+1 = x + iαidi. We want ei+1 ⊥ di.
dT
i ei+1 = dT
i (ei + αidi)=0 
which implies 
dT
i eiαi = − dT dii 
The good news is we can compute everything except the ei. The bad news is computing ei is equivalent 
to ﬁnding x. Fortunately , a mild modiﬁcation will make the calculation possible. 
22-2 So far we’ve been talking about orthogonalit y relativ e to the standard inner product. There’s no real 
reason to do this, and in fact it will be more convenient to work with the inner product ||x||2 = xT Ax,A 
instead of xT Ix as we have been. Geometrically , this unwarps the isolines of the quadratic form into perfect 
circles. 
We can think of this as a change of basis: x/prime = A1/2x, though not for computation – pretty much the 
only way to get the square root of A would be to retriev e the eigenvalues, which would defeat the purpose. 
Suppose we have A-orthogonal search directions (di) – now the unit basis won’t do, but suppose for the 
momen t we have magically acquired search directions. 
Again, xi+1 = xi + αidi. We want ei+1 ⊥A di. 
dT
i Aei+1 = dT
i A(ei + αdi)=0 
which implies 
dT
i Aeiαi = − dT
i Adi 
But Aei is just ri, which we do know how to compute. Yay. 
4 Conjugate Gram-Sc hmidt 
Conjugate directions is insuﬃcien t for our purposes because we might not have time to do n iterations. We’ll 
settle for a crude answer, but we need it very fast. 
Also, as mentioned before, we don’t have search directions. You may recall the Gram-Sc hmidt process 
for orthogonalizing a set of vectors from a previous class. Does it work for A-orthogonalit y? Certainly; see 
page 5 of slides on Conjugate Gram-Sc hmidt. 
The problem is that Conjugate Gram-Sc hmidt is still too slow. The crucial change we made to the 
algorithm is requiring each direction to be orthogonal to all previous search directions. While this gave us 
good convergence, it means we have to subtract oﬀ the projection into each of the previous directions, which 
means that we have to remem ber what the previous directions were. This incurs both time and space cost. 
We need a more sophisticated way to ﬁnd the directions. 
5 Conjugate Gradien ts 
The trick is to choose the linearly independen t vectors we feed to Gram-Sc hmidt very carefully . We will 
generate these vectors on the go. Deﬁne Di = span(d0,...,d i−1). 
The property that we leverage is that after i steps, Conjugate Directions ﬁnds a point in x0 + Di –in 
fact, the one that minimizes the size of the error ||ei||A =(eiT Aei)1/2 . 
Let the input to Gram-Sc hmidt be (ui), and deﬁne Ui analogously to Di. By construction, xi ∈ x0 +Di = 
x0 + Ui, and ei will be A-orthogonal to Di = Ui. 
We choose the magic inputs ui = ri. Since ri+1 = −Aei+1, by deﬁnition ri+1 is plain old orthogonal to 
Di+1 (and Di,Di−1,... ). Also, ri+1 = ri − αiAdi,so Di+1 = Di ∪ ADi. Putting the two together, ri+1 is 
A-orthogonal to Di. 
Thus, ri+1 only A-projects onto the di component of Di+1. There’s only one thing to subtract oﬀ, so 
only one or two A-dot products are needed per iteration again, as in steepest descen t. We no longer need to 
remem ber all the previous search directions, just the very last one, so we’ve ﬁxed the space complexit y as 
well. 
The algorithm is given on a slide on page 6. 
22-3 /summationdisplay 6 Con vergence Analysis of Conjugate Gradien ts 
After i iterations, the error is ⎛ ⎞ 
i 
⎠ ei = ⎝I + ψj Aj e0 
j=1 
where the ψ’s are some mess of α’s and β’s. Thus we can think of conjugate gradien ts at the ith step as 
ﬁnding these best possible coeﬃcien ts for an ith degree polynomial Pi(λ) to make the A-norm of the error 
small. 
||ei||2 
A ≤ min max [Pi(λ)]2||e0||2 
APi λ∈Λ(A) 
Any sequence of i-degree polynomials which are 1 at 0 will give bounds on the error; we want ones which are 
small for every eigenvalue λ ∈ Λ(A). This should remind you of the analysis of steepest descen t, but Taylor 
Series are not the right choice here – they’re designed to work around a point, while we want polynomials 
which will work at every eigenvalue. We can modify the magic polynomials from lecture 6 to work here. 
Recall that Chebyshev polynomials have the property of being 1 at 1, and small for some [0,l] where l< 1 
is a parameter. We want polynomials which are 1 at 0 and small in [λmin,λmax]. This allows us to bound 
the error (measured in the A-inner product) at the ith iteration as 
/parenleftbigg /parenrightbiggi2 ||ei||A ≤ 21 −√ ||e0||A k+1 
so the number iterations grows with the square root of κ, which is way better than the linear performance 
of steepest descen t. 
Note that the algorithm isn’t actually computing any Chebyshev polynomials – it uses the best poly­
nomial, which is at least as good. Also, notice that if we knew the range of the eigenvalues to begin with, 
we could skip to designing a polynomial to estimate A−1 . Conjugate gradien ts magically ﬁnds the best 
polynomial without explicitly knowing these values. 
22-4 MIT OpenCourseWare
http://ocw.mit.edu 
18.409  Topics in Theoretical Computer Science: An Algorithmist's Toolkit 
Fall 2009 
For information about citing these materials or our Terms of Use, visit: http://ocw.mit.edu/terms . 