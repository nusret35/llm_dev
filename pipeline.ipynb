{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting Business Insights Using Generative AI**\n",
    "----\n",
    "In this project, the capabilities of various large language models (LLMs) are utilized to extract business insights from scholarly articles.\n",
    "\n",
    "The focus of this project is to construct a hybrid pipeline that deals with a variety of tasks using various large language models with differing performances. Additionally, prompt engineering is employed to create the best prompts for achieving the highest performance from these models.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import replicate\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined for dividing articles into sections\n",
    "\n",
    "def is_correct_title(title):\n",
    "    for c in title:\n",
    "        if ('a' <= c and c <= 'z') or ('A' <= c and c <= 'Z'):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def divide_article_into_sections(article):\n",
    "    sections = {}\n",
    "    section_titles = re.findall(r'\\d+\\..+?\\n', article)  # Find all lines that start with \"number.title\"\n",
    "    \n",
    "    # Remove the falsely selected section titles\n",
    "    section_titles = [title for title in section_titles if '%' not in title] # EX: '2.5% ...' is not a section title\n",
    "    \n",
    "    # Check the correctness of the chosen section titles by checking the non-decreasing order of the section numbers\n",
    "    correct_titles = []\n",
    "    previous_number = 0  # Start with a sentinel value\n",
    "    for title in section_titles:\n",
    "        pos = article.find(title)\n",
    "        prev_pos = pos-1\n",
    "        # Extract the number at the start of the title\n",
    "        match = re.match(r'(\\d+)(\\.\\d+)?', title)\n",
    "        if match:\n",
    "            number = float(match.group(1))\n",
    "            # Check if the main number (before the dot) is non-decreasing\n",
    "            if number == previous_number or number == (previous_number + 1):\n",
    "                # Check if there is a new line char before the title number\n",
    "                if article[prev_pos] == '\\n':\n",
    "                    if is_correct_title(title):\n",
    "                        correct_titles.append(title)\n",
    "                        previous_number = number\n",
    "                    \n",
    "    section_titles = correct_titles\n",
    "    \n",
    "    # Use zip to pair section titles with their corresponding text\n",
    "    for title, next_title in zip(section_titles, section_titles[1:] + ['']):\n",
    "        # Get the start and end positions of each section\n",
    "        start_pos = article.find(title)\n",
    "        end_pos = article.find(next_title)\n",
    "        \n",
    "        # Extract the section text and remove the section title\n",
    "        section_text = article[start_pos + len(title):end_pos].strip()\n",
    "        \n",
    "        # Store the section in the dictionary with the title as the key\n",
    "        sections[title.strip()] = section_text\n",
    "    \n",
    "    abstract_types = [\"abstract\", \"Abstract\", \"a b s t r a c t\", \"A B S T R A C T\"]\n",
    "    for type in abstract_types:\n",
    "        if type in article:\n",
    "            start_pos = article.find(type)\n",
    "            end_pos = article.find(list(sections.keys())[0])\n",
    "            section_text = article[start_pos + len(type):end_pos]\n",
    "            new_sections_dic = {'abstract':section_text}\n",
    "            new_sections_dic.update(sections)\n",
    "            sections = new_sections_dic\n",
    "            break\n",
    "\n",
    "    # Cleaned sections dictionary\n",
    "    cleaned_sections_dict = {}\n",
    "\n",
    "    # Regular expression to match integers and punctuation\n",
    "    regex = re.compile('[0-9\\.\\,\\!\\?\\:\\;\\-\\â€”\\(\\)]')\n",
    "\n",
    "    for key, value in sections.items():\n",
    "        # Remove integers and punctuation from the key\n",
    "        cleaned_key = regex.sub('', key)\n",
    "        # Convert key to lowercase\n",
    "        cleaned_key = cleaned_key.lower()\n",
    "        # Remove any extra whitespace\n",
    "        cleaned_key = cleaned_key.strip()\n",
    "        # Add to the cleaned dictionary\n",
    "        cleaned_sections_dict[cleaned_key] = value\n",
    "\n",
    "    return cleaned_sections_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions defined for extracting pdf documents and converting them into clean text\n",
    "\n",
    "from pdfminer.high_level import extract_pages \n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTAnno\n",
    "import fitz \n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Function to clean the extracted text.\n",
    "    This function may need to be modified based on the specific 'noises' you encounter in your PDFs.\n",
    "    \"\"\"\n",
    "    # Remove page numbers\n",
    "    text = text[4].split('\\n')\n",
    "    text = [line for line in text if not line.isdigit()]\n",
    "    # Joining text\n",
    "    text = '\\n'.join(text)\n",
    "    return text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file and clean it.\n",
    "    \"\"\"\n",
    "    # Open the provided PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text = \"\"\n",
    "    # Iterate over each page\n",
    "    for page_num in range(len(doc)):   \n",
    "        # Get a page\n",
    "        page = doc.load_page(page_num)\n",
    "        # Extract text from the page\n",
    "        blocks = page.get_text(\"blocks\")\n",
    "        for block in blocks:\n",
    "            # Clean the extracted text\n",
    "            text = clean_text(block)\n",
    "            #print(text)\n",
    "            # Append cleaned text\n",
    "            full_text += text\n",
    "    # Close the document\n",
    "    doc.close()\n",
    "    return full_text\n",
    "\n",
    "def extract_pdf_and_divide_sections(path):\n",
    "    extracted_text = extract_text_from_pdf(path)\n",
    "    #print(extracted_text[20000:30000])\n",
    "    parsed_sections = divide_article_into_sections(extracted_text)\n",
    "    return parsed_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function defined for sending prompt to LLaMA 2 70B model using Replicate's API Service\n",
    "\n",
    "def send_prompt(prompt, sys_prompt):\n",
    "    rp_client = replicate.Client(api_token='r8_VbKuL8aKGq6NNTtMVRRRfHWP6VnCZAl3G2Kum')\n",
    "    output = rp_client.run(\n",
    "        \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n",
    "        input={\n",
    "        \"debug\": False,\n",
    "        \"top_k\": 50,\n",
    "        \"top_p\": 1,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": 0.75,\n",
    "        \"system_prompt\": sys_prompt,\n",
    "        \"max_new_tokens\": 1000,\n",
    "        \"min_new_tokens\": -1\n",
    "    })\n",
    "    response = \"\"\n",
    "    for item in output:\n",
    "        # https://replicate.com/meta/llama-2-70b-chat/versions/02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3/api#output-schema\n",
    "        # print(item, end=\"\")\n",
    "        response += item\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions defined for processes within our pipeline that send prompt to LLaMA 2 70B model\n",
    "\n",
    "def summarize(section_name, section_text):\n",
    "    summarize_sys_prompt = 'You are a tool that summarizes the given text. The given text is a section of an article. Give a concise summary of the section text to include only the most important information.'\n",
    "    prompt = section_name + \": \" + section_text\n",
    "    output = send_prompt(prompt, summarize_sys_prompt)\n",
    "    return output\n",
    "\n",
    "def extract_insights(input):\n",
    "    insights_sys_prompt = 'You are a tool that extracts key insights from an article. You will be provided with article sections. As an output, you should provide concise insights about the given article in bulletpoints.'\n",
    "    prompt = input\n",
    "    output = send_prompt(prompt, insights_sys_prompt)\n",
    "    return output\n",
    "\n",
    "def generate_title(insights):\n",
    "    find_title_sys_prompt = \"From the given insights, provide a title.\"\n",
    "    prompt = \"Extracted insights: \" + insights + \"Title: \"\n",
    "    output = send_prompt(prompt, find_title_sys_prompt)\n",
    "    return output\n",
    "\n",
    "def choose_images(insights, image_titles):\n",
    "    choose_images_sys_prompt = \"Given the image title, choose the most important 3 images of the article based on the insights extracted from the article.\"\n",
    "    prompt = \"Extracted insights: \" + insights + \"Image titles: \" + image_titles + \"Important sections: \"\n",
    "    output = send_prompt(prompt, choose_images_sys_prompt)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting and preprocessing PDF input\n",
    "\n",
    "business_pdf1_path = \"/Users/selinceydeli/Desktop/AIResearch/business-article-inputs/1-s2.0-S0148296323004216-main.pdf\"\n",
    "sections_dict = extract_pdf_and_divide_sections(business_pdf1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting and preprocessing clean text input\n",
    "\n",
    "business_txt_path = \"/Users/selinceydeli/Desktop/AIResearch/llm_dev/summarization_pipeline/bus_article1.txt\"\n",
    "with open(business_txt_path, 'r') as file:\n",
    "    article = file.read()\n",
    "sections_dict = divide_article_into_sections(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting section texts of important sections \n",
    "\n",
    "abstract = sections_dict.get('abstract', \"\")\n",
    "\n",
    "critical_sections = [\"introduction\", \"conclusion\", \"discussion\", \"methodology\"]\n",
    "\n",
    "critical_section_information = {}\n",
    "for section_name in critical_sections:\n",
    "  critical_section_information[section_name] = sections_dict.get(section_name, \"\")\n",
    "\n",
    "\"\"\"\n",
    "If at least two of the sections among \"conclusion\", \"discussion\", and \"outcomes\" are missing, \n",
    "then take the last four sections (we keep each subsection seperately in the current formulation of sections_dict) \n",
    "of the article (excluding keywords, acknowledgments, and references sections)\n",
    "\"\"\"\n",
    "check_for_absence = \"\"\n",
    "critical_section_list = list(critical_section_information.items())\n",
    "for section_name, section_text in critical_section_list[-3:]:\n",
    "    if section_text == \"\": check_for_absence += '0'\n",
    "\n",
    "if len(check_for_absence) >= 2:\n",
    "    accepted = 0\n",
    "    unwanted_sections = [\"keywords\", \"acknowledgments\", \"references\"]\n",
    "    sections_list = list(sections_dict.items())\n",
    "    for section_name, section_text in sections_list[::-1]: # Reverse iteration of the sections_list\n",
    "        section_name = section_name.lower()\n",
    "        section_text = sections_dict.get(section_name, \"\")\n",
    "        if section_name not in unwanted_sections and section_text != \"\":\n",
    "            critical_section_information[section_name] = section_text\n",
    "            accepted += 1\n",
    "            if accepted >= 4:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing important sections\n",
    "    \n",
    "summarized_sections = {}\n",
    "for section_name, section_text in critical_section_information.items():\n",
    "    if section_text != \"\" and section_name != \"introduction\" and section_name != \"managerial implications\": \n",
    "        summary = summarize(section_name, section_text)\n",
    "        summarized_sections[section_name] = summary\n",
    "        print(\"Summary of \" + section_name + \": \\n\" + summary)\n",
    "    else : summarized_sections[section_name] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting insights from the article\n",
    "\n",
    "def create_section_input(summarized_sections):\n",
    "    # Initialize an empty string to store the formatted output\n",
    "    section_input = \"\"\n",
    "\n",
    "    # Iterate over each key-value pair in the dictionary\n",
    "    for key, value in summarized_sections.items():\n",
    "        # Append the key and value to the string with the specified format\n",
    "        section_input += f\"{key}: {value} \\n\"\n",
    "\n",
    "    return section_input\n",
    "\n",
    "section_input = create_section_input(summarized_sections)\n",
    "insights = extract_insights(section_input)\n",
    "print(\"Extracted insights:\\n\" + insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a meaningful title to be presented as the chat title in the interface\n",
    "\n",
    "title = generate_title(insights)\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the most important figures/titles\n",
    "\n",
    "def capture_image_titles(extracted_text):\n",
    "    # Define the possible representations of figure and table names\n",
    "    figure_patterns = [\"Fig\\.\", \"Figure\"]\n",
    "    table_patterns = [\"Table\"]\n",
    "\n",
    "    # Combine the patterns into regex patterns\n",
    "    figure_pattern = \"|\".join(figure_patterns)\n",
    "    table_pattern = \"|\".join(table_patterns)\n",
    "\n",
    "    # Create a regex pattern to capture the figure and table titles\n",
    "    figure_title_pattern = f\"({figure_pattern})\\s*(\\d+)\\.\\s*(.*?)\\.\"\n",
    "    table_title_pattern = f\"({table_pattern})\\s*(\\d+)\\s*\\\\n?\\s*(.*?)\\.\"\n",
    "\n",
    "    # Find all matches in the extracted_text for figures and tables\n",
    "    figure_matches = re.findall(figure_title_pattern, extracted_text)\n",
    "    table_matches = re.findall(table_title_pattern, extracted_text)\n",
    "\n",
    "    # Initialize lists to store figure and table titles\n",
    "    titles = []\n",
    "\n",
    "    # Process and store the matched titles and numberings from the figures\n",
    "    for match in figure_matches:\n",
    "        title_type, title_number, title_text = match\n",
    "        if 'A' <= title_text[0] <= \"Z\":\n",
    "            titles.append(f\"{title_type} {title_number}. {title_text}\")\n",
    "\n",
    "    # Process and store the matched titles and numberings from the tables\n",
    "    for match in table_matches:\n",
    "        title_type, title_number, title_text = match\n",
    "        if title_text and 'A' <= title_text[0] <= \"Z\":\n",
    "            titles.append(f\"{title_type} {title_number}. {title_text}\")\n",
    "\n",
    "    return titles\n",
    "\n",
    "def extract_pdf(path):\n",
    "    extracted_text = extract_text_from_pdf(path)\n",
    "    return extracted_text\n",
    "\n",
    "extracted_pdf = extract_pdf(business_pdf1_path)\n",
    "titles = capture_image_titles(extracted_pdf)\n",
    "image_titles = \"\"\n",
    "for title in titles:\n",
    "    image_titles += title + \"\\n\"\n",
    "important_images = choose_images(insights, image_titles)\n",
    "print(important_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "\n",
    "def extract_titles_from_page(page):\n",
    "    \n",
    "    # Define the possible representations of figure and table names\n",
    "    figure_patterns = [\"Fig\\.\", \"Figure\"]\n",
    "    table_patterns = [\"Table\"]\n",
    "\n",
    "    # Combine the patterns into regex patterns\n",
    "    figure_pattern = \"|\".join(figure_patterns)\n",
    "    table_pattern = \"|\".join(table_patterns)\n",
    "\n",
    "    # Create a regex pattern to capture the figure and table titles\n",
    "    figure_title_pattern = f\"({figure_pattern})\\s*(\\d+)\\.\\s*(.*?)\\.\"\n",
    "    table_title_pattern = f\"({table_pattern})\\s*(\\d+)\\s*\\\\n?\\s*(.*?)\\.\"\n",
    "\n",
    "    text_blocks = page.get_text(\"blocks\")\n",
    "\n",
    "    # Initialize lists to store figure and table titles\n",
    "    titles = []\n",
    "    \n",
    "    for block in text_blocks:\n",
    "        block_text = block[4]\n",
    "        \n",
    "        # Find all matches in the extracted_text for figures and tables\n",
    "        figure_matches = re.findall(figure_title_pattern, block_text)\n",
    "        table_matches = re.findall(table_title_pattern, block_text)\n",
    "\n",
    "        # Process and store the matched titles and numberings from the figures\n",
    "        for match in figure_matches:\n",
    "            title_type, title_number, title_text = match\n",
    "            if title_text != \"\":\n",
    "                if ('A' <= title_text[0] and title_text[0] <= \"Z\") or ('0' <= title_text[0] and title_text[0] <= '9'):\n",
    "                    titles.append(f\"{title_type} {title_number}. {title_text}\") \n",
    "\n",
    "        # Process and store the matched titles and numberings from the tables\n",
    "        for match in table_matches:\n",
    "            title_type, title_number, title_text = match\n",
    "            if title_text != \"\":\n",
    "                if ('A' <= title_text[0] and title_text[0] <= \"Z\") or ('0' <= title_text[0] and title_text[0] <= '9'):\n",
    "                    titles.append(f\"{title_type} {title_number}. {title_text}\")\n",
    "\n",
    "    return titles\n",
    "\n",
    "# Open the file\n",
    "pdf_file = fitz.open(business_pdf1_path)\n",
    "\n",
    "titles = []\n",
    "\n",
    "# Iterate over PDF pages\n",
    "for page_index in range(len(pdf_file)):\n",
    "    page = pdf_file[page_index]\n",
    "    page_image_titles = extract_titles_from_page(page)\n",
    "    for title in page_image_titles:\n",
    "        title += \" (Page:\" + str(page_index+1) + \")\"\n",
    "        titles.append(title)\n",
    "\n",
    "pdf_file.close()\n",
    "\n",
    "image_titles = \"\"\n",
    "for title in titles:\n",
    "    image_titles += title + \"\\n\"\n",
    "important_images = choose_images(insights, image_titles)\n",
    "print(important_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
